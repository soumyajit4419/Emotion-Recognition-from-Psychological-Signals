{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScHoIgpVSmnS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c69e9b4-5864-48e8-d763-7972271659d0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxDcyW_lKVYN"
      },
      "source": [
        "import extract_vector as ev\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,DataLoader,random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqgAo_DBKmsm"
      },
      "source": [
        "def clean_train_data(train_data, train_label):\n",
        "    n = train_data.shape[0]\n",
        "    m = train_data.shape[1]\n",
        "    nan_ll = []\n",
        "    for i in range(n):\n",
        "        if(np.isnan(np.sum(train_data[i:i+1, :]))):\n",
        "            nan_ll.append(i)\n",
        "\n",
        "    train_data = np.delete(train_data, nan_ll, 0)\n",
        "    train_label = np.delete(train_label, nan_ll, 0)\n",
        "    return train_data, train_label\n",
        "\n",
        "def get_train_labels(train_label):\n",
        "    for i in train_label:\n",
        "        if(i[0] > 4.5):\n",
        "            i[0] = 1\n",
        "        else:\n",
        "            i[0] = 0\n",
        "\n",
        "        if(i[1] > 4.5):\n",
        "            i[1] = 1\n",
        "        else:\n",
        "            i[1] = 0\n",
        "    return train_label\n",
        "\n",
        "def get_emotion_label(labels):\n",
        "    emo = []\n",
        "    for i in labels:\n",
        "        if(i[0] == 0 and i[1] == 0):\n",
        "            emo.append(0)\n",
        "        elif(i[0] == 1 and i[1] == 0):\n",
        "            emo.append(1)\n",
        "        elif(i[0] == 0 and i[1] == 1):\n",
        "            emo.append(2)\n",
        "        elif(i[0] == 1 and i[1] == 1):\n",
        "            emo.append(3)\n",
        "    return emo"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxK-OuYAU8Db"
      },
      "source": [
        "class Entropy_Model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(14,64)\n",
        "    self.linear2 = nn.Linear(64,128)\n",
        "    self.linear3 = nn.Linear(128,256)\n",
        "    self.linear4 = nn.Linear(256,1)\n",
        "  \n",
        "  def forward(self,xb):\n",
        "    out = self.linear1(xb)\n",
        "    out = F.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.linear3(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.linear4(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out\n",
        "  \n",
        "  def training_step(self,batch):\n",
        "    features,label = batch\n",
        "    out = self(features)\n",
        "    loss = F.binary_cross_entropy(out,label)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self,batch):\n",
        "    features,label = batch\n",
        "    out = self(features)\n",
        "    loss = F.binary_cross_entropy(out,label)\n",
        "    acc = accuracy(out,label)\n",
        "    return {\"val_loss\": loss.detach(),\"val_acc\": acc}\n",
        "\n",
        "  def validation_epoch_end(self,outputs):\n",
        "    batch_loss = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_loss).mean()\n",
        "    batch_acc = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_acc).mean()\n",
        "    return {\"val_loss\":epoch_loss.item(),\"val_acc\":epoch_acc.item()}\n",
        "\n",
        "  def epoch_end(self,num_epoch,results):\n",
        "    print(\"num_epoch: {}, train_loss: {:.2f}, val_loss: {:.2f}, val_acc: {:.2f}\".format(num_epoch+1,results['train_loss'],results['val_loss'], results['val_acc']))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TDS0VePU8Iy"
      },
      "source": [
        "def accuracy(out,label):\n",
        "  out = (out>0.5)\n",
        "  pred = (out == label).sum()\n",
        "  return pred/out.shape[0]\n",
        "\n",
        "def evaluate(model,val_loader):\n",
        "  outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(num_epochs,lr,train_loader,val_loader,model,opt_func=torch.optim.Adam):\n",
        "  optimizer = opt_func(model.parameters(),lr)\n",
        "  history = []\n",
        "  for epoch in range(num_epochs):\n",
        "    train_losses = []\n",
        "    for batch in train_loader:\n",
        "      loss = model.training_step(batch)\n",
        "      train_losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    results = evaluate(model,val_loader)\n",
        "    train_loss = torch.stack(train_losses).mean().item()\n",
        "    results['train_loss'] = train_loss\n",
        "    model.epoch_end(epoch,results)\n",
        "    history.append(results)\n",
        "  return history"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w7AiqRDJypc"
      },
      "source": [
        "## Wavelet Entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLO9YMUtKVbC"
      },
      "source": [
        "train_data = ev.getWaveletEntropyData()\n",
        "train_label = ev.getLabelData(type='ValAr')\n",
        "train_data,train_label = clean_train_data(train_data,train_label)\n",
        "train_label = get_train_labels(train_label)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OZNF9_3d_D_"
      },
      "source": [
        "train_data = torch.tensor(train_data,dtype=torch.float32)\n",
        "train_label = torch.tensor(train_label,dtype=torch.float32)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFjm8YjyLeXs"
      },
      "source": [
        "arousal_dataset = TensorDataset(train_data,train_label[:,0].unsqueeze(1))\n",
        "valence_dataset = TensorDataset(train_data,train_label[:,1].unsqueeze(1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLzEpGkuKvQa"
      },
      "source": [
        "### Arousal Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdvXK9S7KVdy"
      },
      "source": [
        "def split_data(dataset):\n",
        "  test_size = int(len(dataset) * 0.2)\n",
        "  train_size = len(dataset) - test_size\n",
        "  train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
        "  return train_ds,test_ds\n",
        "\n",
        "train_arousal,test_arousal = split_data(arousal_dataset)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_arousal,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(test_arousal,batch_size = batch_size,shuffle=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMhFwNs20n9A"
      },
      "source": [
        "arousal_model = Entropy_Model()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU6e7m0MpMYr",
        "outputId": "019f5872-d7cb-4172-ed61-6a7ff58271d5"
      },
      "source": [
        "history = fit(10,0.001,train_loader,test_loader,arousal_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epoch: 1, train_loss: 0.66, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 2, train_loss: 0.64, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 3, train_loss: 0.64, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 4, train_loss: 0.64, val_loss: 0.60, val_acc: 0.71\n",
            "num_epoch: 5, train_loss: 0.64, val_loss: 0.61, val_acc: 0.70\n",
            "num_epoch: 6, train_loss: 0.64, val_loss: 0.62, val_acc: 0.71\n",
            "num_epoch: 7, train_loss: 0.64, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 8, train_loss: 0.64, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 9, train_loss: 0.64, val_loss: 0.61, val_acc: 0.71\n",
            "num_epoch: 10, train_loss: 0.64, val_loss: 0.61, val_acc: 0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuZAWBeLJaqt",
        "outputId": "8a6dab7c-84bd-499c-84d8-e4cefd93413c"
      },
      "source": [
        "evaluate(arousal_model,test_loader)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.7054347991943359, 'val_loss': 0.6093981862068176}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzY9UsKaLMvq"
      },
      "source": [
        "### Valence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxuF40HZK5uH"
      },
      "source": [
        "def split_data(dataset):\n",
        "  test_size = int(len(dataset) * 0.2)\n",
        "  train_size = len(dataset) - test_size\n",
        "  train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
        "  return train_ds,test_ds\n",
        "\n",
        "train_valence,test_valence = split_data(valence_dataset)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_valence,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(test_valence,batch_size = batch_size,shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SF6_BqfK5xY"
      },
      "source": [
        "valence_model = Entropy_Model()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MArHBntlLTM-",
        "outputId": "46031758-2818-4923-90af-d3a11a92395d"
      },
      "source": [
        "history = fit(10,0.001,train_loader,test_loader,valence_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epoch: 1, train_loss: 0.68, val_loss: 0.68, val_acc: 0.58\n",
            "num_epoch: 2, train_loss: 0.67, val_loss: 0.69, val_acc: 0.57\n",
            "num_epoch: 3, train_loss: 0.67, val_loss: 0.69, val_acc: 0.57\n",
            "num_epoch: 4, train_loss: 0.67, val_loss: 0.68, val_acc: 0.59\n",
            "num_epoch: 5, train_loss: 0.67, val_loss: 0.68, val_acc: 0.58\n",
            "num_epoch: 6, train_loss: 0.67, val_loss: 0.68, val_acc: 0.58\n",
            "num_epoch: 7, train_loss: 0.66, val_loss: 0.68, val_acc: 0.58\n",
            "num_epoch: 8, train_loss: 0.66, val_loss: 0.69, val_acc: 0.57\n",
            "num_epoch: 9, train_loss: 0.66, val_loss: 0.68, val_acc: 0.58\n",
            "num_epoch: 10, train_loss: 0.66, val_loss: 0.68, val_acc: 0.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwbd2dVwLXE2",
        "outputId": "12301f23-67cb-4c84-b2ef-24cc0877a655"
      },
      "source": [
        "evaluate(valence_model,test_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.5755435228347778, 'val_loss': 0.6777989864349365}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RILqp1HXLkck"
      },
      "source": [
        "## Fourier Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfPi1YmpLrom"
      },
      "source": [
        "train_data = ev.getFourierEntropyData()\n",
        "train_label = ev.getLabelData(type='ValAr')\n",
        "train_data,train_label = clean_train_data(train_data,train_label)\n",
        "train_label = get_train_labels(train_label)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RQsGoETLrqq"
      },
      "source": [
        "train_data = torch.tensor(train_data,dtype=torch.float32)\n",
        "train_label = torch.tensor(train_label,dtype=torch.float32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gUy8b7AMgLL"
      },
      "source": [
        "arousal_dataset = TensorDataset(train_data,train_label[:,0].unsqueeze(1))\n",
        "valence_dataset = TensorDataset(train_data,train_label[:,1].unsqueeze(1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPZ044OJLqry"
      },
      "source": [
        "### Arousal Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kckpNzz6LefF"
      },
      "source": [
        "def split_data(dataset):\n",
        "  test_size = int(len(dataset) * 0.2)\n",
        "  train_size = len(dataset) - test_size\n",
        "  train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
        "  return train_ds,test_ds\n",
        "\n",
        "train_arousal,test_arousal = split_data(arousal_dataset)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_arousal,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(test_arousal,batch_size = batch_size,shuffle=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWGPf7fsLeiF"
      },
      "source": [
        "arousal_model = Entropy_Model()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfEwUHR0Leku",
        "outputId": "96369f9e-a2f6-49b9-c73d-3053f1324f85"
      },
      "source": [
        "history = fit(10,0.001,train_loader,test_loader,arousal_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epoch: 1, train_loss: 0.66, val_loss: 0.64, val_acc: 0.67\n",
            "num_epoch: 2, train_loss: 0.63, val_loss: 0.63, val_acc: 0.67\n",
            "num_epoch: 3, train_loss: 0.64, val_loss: 0.64, val_acc: 0.67\n",
            "num_epoch: 4, train_loss: 0.63, val_loss: 0.64, val_acc: 0.67\n",
            "num_epoch: 5, train_loss: 0.63, val_loss: 0.63, val_acc: 0.68\n",
            "num_epoch: 6, train_loss: 0.63, val_loss: 0.64, val_acc: 0.67\n",
            "num_epoch: 7, train_loss: 0.63, val_loss: 0.63, val_acc: 0.68\n",
            "num_epoch: 8, train_loss: 0.63, val_loss: 0.63, val_acc: 0.67\n",
            "num_epoch: 9, train_loss: 0.63, val_loss: 0.63, val_acc: 0.68\n",
            "num_epoch: 10, train_loss: 0.63, val_loss: 0.63, val_acc: 0.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPYjgMcuMtqQ",
        "outputId": "8de209f4-da66-45a7-881d-e5c5bde20473"
      },
      "source": [
        "evaluate(arousal_model,test_loader)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.667934775352478, 'val_loss': 0.6355298757553101}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3E-3G96Lwcx"
      },
      "source": [
        "### Valence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As0qEEgpLenJ"
      },
      "source": [
        "def split_data(dataset):\n",
        "  test_size = int(len(dataset) * 0.2)\n",
        "  train_size = len(dataset) - test_size\n",
        "  train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
        "  return train_ds,test_ds\n",
        "\n",
        "train_valence,test_valence = split_data(valence_dataset)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_valence,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(test_valence,batch_size = batch_size,shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWdUO9ajL3II"
      },
      "source": [
        "valence_model = Entropy_Model()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvALk6M8L3Kf",
        "outputId": "f17a980a-116a-4306-b7cd-86e90767c6dd"
      },
      "source": [
        "history = fit(10,0.001,train_loader,test_loader,valence_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epoch: 1, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
            "num_epoch: 2, train_loss: 0.68, val_loss: 0.67, val_acc: 0.62\n",
            "num_epoch: 3, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
            "num_epoch: 4, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
            "num_epoch: 5, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
            "num_epoch: 6, train_loss: 0.67, val_loss: 0.67, val_acc: 0.62\n",
            "num_epoch: 7, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
            "num_epoch: 8, train_loss: 0.67, val_loss: 0.67, val_acc: 0.62\n",
            "num_epoch: 9, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
            "num_epoch: 10, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7iWRB_vMuUr",
        "outputId": "712f25c1-5989-4f98-ba5a-59d8bf59ff2e"
      },
      "source": [
        "evaluate(valence_model,test_loader)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.632880449295044, 'val_loss': 0.6582896113395691}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}