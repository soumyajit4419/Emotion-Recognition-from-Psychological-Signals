{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ScHoIgpVSmnS",
    "outputId": "183f78d7-7dbe-434d-ace1-b2a8af3696fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nxDcyW_lKVYN"
   },
   "outputs": [],
   "source": [
    "import extract_vector as ev\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mqgAo_DBKmsm"
   },
   "outputs": [],
   "source": [
    "def clean_train_data(train_data, train_label):\n",
    "    n = train_data.shape[0]\n",
    "    m = train_data.shape[1]\n",
    "    nan_ll = []\n",
    "    for i in range(n):\n",
    "        if(np.isnan(np.sum(train_data[i:i+1, :]))):\n",
    "            nan_ll.append(i)\n",
    "\n",
    "    train_data = np.delete(train_data, nan_ll, 0)\n",
    "    train_label = np.delete(train_label, nan_ll, 0)\n",
    "    return train_data, train_label\n",
    "\n",
    "def get_train_labels(train_label):\n",
    "    for i in train_label:\n",
    "        if(i[0] > 4.5):\n",
    "            i[0] = 1\n",
    "        else:\n",
    "            i[0] = 0\n",
    "\n",
    "        if(i[1] > 4.5):\n",
    "            i[1] = 1\n",
    "        else:\n",
    "            i[1] = 0\n",
    "            \n",
    "        if(i[2] > 4.5):\n",
    "            i[2] = 1\n",
    "        else:\n",
    "            i[2] = 0\n",
    "            \n",
    "        if(i[3] > 4.5):\n",
    "            i[3] = 1\n",
    "        else:\n",
    "            i[3] = 0\n",
    "    return train_label\n",
    "\n",
    "def get_emotion_label(labels):\n",
    "    emo = []\n",
    "    for i in labels:\n",
    "        if(i[0] == 0 and i[1] == 0):\n",
    "            emo.append(0)\n",
    "        elif(i[0] == 1 and i[1] == 0):\n",
    "            emo.append(1)\n",
    "        elif(i[0] == 0 and i[1] == 1):\n",
    "            emo.append(2)\n",
    "        elif(i[0] == 1 and i[1] == 1):\n",
    "            emo.append(3)\n",
    "    return emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YxK-OuYAU8Db"
   },
   "outputs": [],
   "source": [
    "class Entropy_Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(14,64)\n",
    "        self.linear2 = nn.Linear(64,128)\n",
    "        self.linear3 = nn.Linear(128,256)\n",
    "        self.linear4 = nn.Linear(256,512)\n",
    "        self.linear5 = nn.Linear(512,1)\n",
    "  \n",
    "    def forward(self,xb):\n",
    "        out = self.linear1(xb)\n",
    "        out = F.elu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.elu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = F.elu(out)\n",
    "        out = self.linear4(out)\n",
    "        out = F.elu(out)\n",
    "        out = self.linear5(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "  \n",
    "    def training_step(self,batch):\n",
    "        features,label = batch\n",
    "        out = self(features)\n",
    "        loss = F.binary_cross_entropy(out,label)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch):\n",
    "        features,label = batch\n",
    "        out = self(features)\n",
    "        loss = F.binary_cross_entropy(out,label)\n",
    "        acc = accuracy(out,label)\n",
    "        return {\"val_loss\": loss.detach(),\"val_acc\": acc}\n",
    "\n",
    "    def validation_epoch_end(self,outputs):\n",
    "        batch_loss = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_loss).mean()\n",
    "        batch_acc = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        return {\"val_loss\":epoch_loss.item(),\"val_acc\":epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self,num_epoch,results):\n",
    "        print(\"num_epoch: {}, train_loss: {:.2f}, val_loss: {:.2f}, val_acc: {:.2f}\".format(num_epoch+1,results['train_loss'],results['val_loss'], results['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9TDS0VePU8Iy"
   },
   "outputs": [],
   "source": [
    "def accuracy(out,label):\n",
    "    out = (out>0.5)\n",
    "    pred = (out == label).sum()\n",
    "    return pred/out.shape[0]\n",
    "\n",
    "def evaluate(model,val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(num_epochs,lr,train_loader,val_loader,model,opt_func=torch.optim.Adam):\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        results = evaluate(model,val_loader)\n",
    "        train_loss = torch.stack(train_losses).mean().item()\n",
    "        results['train_loss'] = train_loss\n",
    "        model.epoch_end(epoch,results)\n",
    "        history.append(results)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w7AiqRDJypc"
   },
   "source": [
    "## Wavelet Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bLO9YMUtKVbC"
   },
   "outputs": [],
   "source": [
    "train_data = ev.getWaveletEntropyData()\n",
    "train_label = ev.getLabelData(type='allfour')\n",
    "train_data,train_label = clean_train_data(train_data,train_label)\n",
    "train_label = get_train_labels(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2OZNF9_3d_D_"
   },
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data,dtype=torch.float32)\n",
    "train_label = torch.tensor(train_label,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZFjm8YjyLeXs"
   },
   "outputs": [],
   "source": [
    "arousal_dataset = TensorDataset(train_data,train_label[:,0].unsqueeze(1))\n",
    "valence_dataset = TensorDataset(train_data,train_label[:,1].unsqueeze(1))\n",
    "dominance_dataset=TensorDataset(train_data,train_label[:,2].unsqueeze(1))\n",
    "liking_dataset=TensorDataset(train_data,train_label[:,3].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 1., 1.],\n",
       "        ...,\n",
       "        [0., 1., 1., 0.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLzEpGkuKvQa"
   },
   "source": [
    "### Arousal Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "qdvXK9S7KVdy"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_arousal,test_arousal = split_data(arousal_dataset)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_arousal,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_arousal,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "EMhFwNs20n9A"
   },
   "outputs": [],
   "source": [
    "arousal_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EU6e7m0MpMYr",
    "outputId": "6e4e64ce-c880-4f33-b2d8-a65a63e0d3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.66, val_loss: 0.61, val_acc: 0.74\n",
      "num_epoch: 2, train_loss: 0.64, val_loss: 0.59, val_acc: 0.74\n",
      "num_epoch: 3, train_loss: 0.64, val_loss: 0.59, val_acc: 0.74\n",
      "num_epoch: 4, train_loss: 0.65, val_loss: 0.60, val_acc: 0.73\n",
      "num_epoch: 5, train_loss: 0.64, val_loss: 0.60, val_acc: 0.74\n",
      "num_epoch: 6, train_loss: 0.64, val_loss: 0.60, val_acc: 0.73\n",
      "num_epoch: 7, train_loss: 0.64, val_loss: 0.60, val_acc: 0.72\n",
      "num_epoch: 8, train_loss: 0.65, val_loss: 0.58, val_acc: 0.75\n",
      "num_epoch: 9, train_loss: 0.64, val_loss: 0.59, val_acc: 0.75\n",
      "num_epoch: 10, train_loss: 0.64, val_loss: 0.59, val_acc: 0.75\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.0001,train_loader,test_loader,arousal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuZAWBeLJaqt",
    "outputId": "5257d6b5-8f12-42ae-86bb-65eaa6e8a0c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.5913291573524475, 'val_acc': 0.7359601855278015}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(arousal_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzY9UsKaLMvq"
   },
   "source": [
    "### Valence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "DxuF40HZK5uH"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_valence,test_valence = split_data(valence_dataset)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_valence,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_valence,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "7SF6_BqfK5xY"
   },
   "outputs": [],
   "source": [
    "valence_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MArHBntlLTM-",
    "outputId": "ce16d41a-8187-4d58-f883-6874d9b28477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.68, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 2, train_loss: 0.68, val_loss: 0.65, val_acc: 0.64\n",
      "num_epoch: 3, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 4, train_loss: 0.67, val_loss: 0.68, val_acc: 0.64\n",
      "num_epoch: 5, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 6, train_loss: 0.68, val_loss: 0.67, val_acc: 0.64\n",
      "num_epoch: 7, train_loss: 0.68, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 8, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 9, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 10, train_loss: 0.68, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 11, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 12, train_loss: 0.68, val_loss: 0.67, val_acc: 0.64\n",
      "num_epoch: 13, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 14, train_loss: 0.68, val_loss: 0.67, val_acc: 0.65\n",
      "num_epoch: 15, train_loss: 0.68, val_loss: 0.65, val_acc: 0.64\n",
      "num_epoch: 16, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 17, train_loss: 0.67, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 18, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 19, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 20, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 21, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 22, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 23, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 24, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 25, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n"
     ]
    }
   ],
   "source": [
    "history = fit(25,0.001,train_loader,test_loader,valence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uwbd2dVwLXE2",
    "outputId": "5642e65c-6c3c-40cf-e04d-5d21b7f1b785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6555060148239136, 'val_acc': 0.647826075553894}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(valence_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_dominance,test_dominance = split_data(dominance_dataset)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dominance,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dominance,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.69, val_loss: 0.68, val_acc: 0.58\n",
      "num_epoch: 2, train_loss: 0.69, val_loss: 0.68, val_acc: 0.60\n",
      "num_epoch: 3, train_loss: 0.69, val_loss: 0.68, val_acc: 0.61\n",
      "num_epoch: 4, train_loss: 0.69, val_loss: 0.68, val_acc: 0.60\n",
      "num_epoch: 5, train_loss: 0.69, val_loss: 0.69, val_acc: 0.60\n",
      "num_epoch: 6, train_loss: 0.69, val_loss: 0.68, val_acc: 0.61\n",
      "num_epoch: 7, train_loss: 0.69, val_loss: 0.68, val_acc: 0.62\n",
      "num_epoch: 8, train_loss: 0.69, val_loss: 0.68, val_acc: 0.60\n",
      "num_epoch: 9, train_loss: 0.69, val_loss: 0.69, val_acc: 0.60\n",
      "num_epoch: 10, train_loss: 0.69, val_loss: 0.68, val_acc: 0.62\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.0001,train_loader,test_loader,dominance_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6813305616378784, 'val_acc': 0.6008928418159485}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dominance_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liking Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_liking,test_liking = split_data(liking_dataset)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_liking,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_liking,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "liking_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.62, val_loss: 0.54, val_acc: 0.78\n",
      "num_epoch: 2, train_loss: 0.61, val_loss: 0.54, val_acc: 0.78\n",
      "num_epoch: 3, train_loss: 0.60, val_loss: 0.55, val_acc: 0.77\n",
      "num_epoch: 4, train_loss: 0.60, val_loss: 0.54, val_acc: 0.77\n",
      "num_epoch: 5, train_loss: 0.60, val_loss: 0.55, val_acc: 0.77\n",
      "num_epoch: 6, train_loss: 0.60, val_loss: 0.54, val_acc: 0.78\n",
      "num_epoch: 7, train_loss: 0.60, val_loss: 0.55, val_acc: 0.77\n",
      "num_epoch: 8, train_loss: 0.60, val_loss: 0.54, val_acc: 0.77\n",
      "num_epoch: 9, train_loss: 0.60, val_loss: 0.54, val_acc: 0.78\n",
      "num_epoch: 10, train_loss: 0.60, val_loss: 0.54, val_acc: 0.78\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.0001,train_loader,test_loader,liking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.5536023378372192, 'val_acc': 0.7630435228347778}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(liking_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RILqp1HXLkck"
   },
   "source": [
    "## Fourier Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bfPi1YmpLrom"
   },
   "outputs": [],
   "source": [
    "train_data = ev.getFourierEntropyData()\n",
    "train_label = ev.getLabelData(type='allfour')\n",
    "train_data,train_label = clean_train_data(train_data,train_label)\n",
    "train_label = get_train_labels(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_RQsGoETLrqq"
   },
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data,dtype=torch.float32)\n",
    "train_label = torch.tensor(train_label,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0gUy8b7AMgLL"
   },
   "outputs": [],
   "source": [
    "arousal_dataset = TensorDataset(train_data,train_label[:,0].unsqueeze(1))\n",
    "valence_dataset = TensorDataset(train_data,train_label[:,1].unsqueeze(1))\n",
    "dominance_dataset=TensorDataset(train_data,train_label[:,2].unsqueeze(1))\n",
    "liking_dataset=TensorDataset(train_data,train_label[:,3].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPZ044OJLqry"
   },
   "source": [
    "### Arousal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kckpNzz6LefF"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_arousal,test_arousal = split_data(arousal_dataset)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_arousal,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_arousal,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QWGPf7fsLeiF"
   },
   "outputs": [],
   "source": [
    "arousal_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfEwUHR0Leku",
    "outputId": "2d8a39c2-6c25-40ae-f296-b96f0d4266ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.66, val_loss: 0.62, val_acc: 0.70\n",
      "num_epoch: 2, train_loss: 0.64, val_loss: 0.62, val_acc: 0.70\n",
      "num_epoch: 3, train_loss: 0.64, val_loss: 0.62, val_acc: 0.70\n",
      "num_epoch: 4, train_loss: 0.64, val_loss: 0.62, val_acc: 0.69\n",
      "num_epoch: 5, train_loss: 0.64, val_loss: 0.62, val_acc: 0.69\n",
      "num_epoch: 6, train_loss: 0.64, val_loss: 0.61, val_acc: 0.70\n",
      "num_epoch: 7, train_loss: 0.64, val_loss: 0.62, val_acc: 0.69\n",
      "num_epoch: 8, train_loss: 0.64, val_loss: 0.62, val_acc: 0.70\n",
      "num_epoch: 9, train_loss: 0.64, val_loss: 0.62, val_acc: 0.69\n",
      "num_epoch: 10, train_loss: 0.64, val_loss: 0.61, val_acc: 0.70\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.0001,train_loader,test_loader,arousal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPYjgMcuMtqQ",
    "outputId": "d3749e0e-df7b-4ffc-dc95-abf3f9134968"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6091545820236206, 'val_acc': 0.7051630616188049}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(arousal_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3E-3G96Lwcx"
   },
   "source": [
    "### Valence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "As0qEEgpLenJ"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_valence,test_valence = split_data(valence_dataset)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_valence,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_valence,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LWdUO9ajL3II"
   },
   "outputs": [],
   "source": [
    "valence_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvALk6M8L3Kf",
    "outputId": "04bffb2c-a266-4ead-cd86-3d61f5387c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.69, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 2, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 3, train_loss: 0.68, val_loss: 0.66, val_acc: 0.63\n",
      "num_epoch: 4, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 5, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 6, train_loss: 0.68, val_loss: 0.65, val_acc: 0.64\n",
      "num_epoch: 7, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 8, train_loss: 0.68, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 9, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
      "num_epoch: 10, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 11, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
      "num_epoch: 12, train_loss: 0.67, val_loss: 0.66, val_acc: 0.63\n",
      "num_epoch: 13, train_loss: 0.67, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 14, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 15, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 16, train_loss: 0.67, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 17, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 18, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 19, train_loss: 0.68, val_loss: 0.66, val_acc: 0.65\n",
      "num_epoch: 20, train_loss: 0.68, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 21, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 22, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 23, train_loss: 0.68, val_loss: 0.65, val_acc: 0.65\n",
      "num_epoch: 24, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n",
      "num_epoch: 25, train_loss: 0.67, val_loss: 0.66, val_acc: 0.64\n"
     ]
    }
   ],
   "source": [
    "history = fit(25,0.0001,train_loader,test_loader,valence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7iWRB_vMuUr",
    "outputId": "62512cd3-59c4-41e7-f3c5-55255c4ad291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6581810712814331, 'val_acc': 0.6380435228347778}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(valence_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dominance,test_dominance = split_data(dominance_dataset)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dominance,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dominance,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.70, val_loss: 0.69, val_acc: 0.56\n",
      "num_epoch: 2, train_loss: 0.69, val_loss: 0.68, val_acc: 0.58\n",
      "num_epoch: 3, train_loss: 0.69, val_loss: 0.68, val_acc: 0.59\n",
      "num_epoch: 4, train_loss: 0.69, val_loss: 0.68, val_acc: 0.58\n",
      "num_epoch: 5, train_loss: 0.69, val_loss: 0.68, val_acc: 0.57\n",
      "num_epoch: 6, train_loss: 0.69, val_loss: 0.69, val_acc: 0.57\n",
      "num_epoch: 7, train_loss: 0.69, val_loss: 0.69, val_acc: 0.58\n",
      "num_epoch: 8, train_loss: 0.69, val_loss: 0.68, val_acc: 0.58\n",
      "num_epoch: 9, train_loss: 0.69, val_loss: 0.69, val_acc: 0.57\n",
      "num_epoch: 10, train_loss: 0.69, val_loss: 0.68, val_acc: 0.58\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.001,train_loader,test_loader,dominance_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6846518516540527, 'val_acc': 0.5682064890861511}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dominance_model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    test_size = int(len(dataset) * 0.2)\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_ds,test_ds = random_split(dataset,[train_size,test_size])\n",
    "    return train_ds,test_ds\n",
    "\n",
    "train_liking,test_liking = split_data(liking_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_liking,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_liking,batch_size = batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "liking_model = Entropy_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch: 1, train_loss: 0.63, val_loss: 0.60, val_acc: 0.71\n",
      "num_epoch: 2, train_loss: 0.59, val_loss: 0.61, val_acc: 0.71\n",
      "num_epoch: 3, train_loss: 0.59, val_loss: 0.61, val_acc: 0.70\n",
      "num_epoch: 4, train_loss: 0.59, val_loss: 0.61, val_acc: 0.71\n",
      "num_epoch: 5, train_loss: 0.59, val_loss: 0.60, val_acc: 0.72\n",
      "num_epoch: 6, train_loss: 0.59, val_loss: 0.60, val_acc: 0.71\n",
      "num_epoch: 7, train_loss: 0.59, val_loss: 0.60, val_acc: 0.72\n",
      "num_epoch: 8, train_loss: 0.59, val_loss: 0.61, val_acc: 0.70\n",
      "num_epoch: 9, train_loss: 0.59, val_loss: 0.60, val_acc: 0.71\n",
      "num_epoch: 10, train_loss: 0.59, val_loss: 0.61, val_acc: 0.71\n"
     ]
    }
   ],
   "source": [
    "history = fit(10,0.0001,train_loader,test_loader,liking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.602447509765625, 'val_acc': 0.710326075553894}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(liking_model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
